{"cells":[{"metadata":{"colab":{},"colab_type":"code","id":"g9_aGJP3tvF4","trusted":true},"cell_type":"code","source":"import glob\nimport os\nimport librosa\nimport numpy as np\nimport seaborn as sns\nimport wave\nimport contextlib\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score,classification_report,f1_score\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\n%matplotlib inline\nplt.style.use('ggplot')\n\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.labelsize'] = 11\nplt.rcParams['axes.labelweight'] = 'bold'\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['xtick.labelsize'] = 10\nplt.rcParams['ytick.labelsize'] = 10\nplt.rcParams['legend.fontsize'] = 11\nplt.rcParams['figure.titlesize'] = 13","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"ovHpIKfEtvGG"},"cell_type":"markdown","source":" # Features plots - Code"},{"metadata":{"colab":{},"colab_type":"code","id":"CGCfJPgjtvGM","trusted":true},"cell_type":"code","source":"def load_sound_files(file_paths):\n    raw_sounds = []\n    for fp in file_paths:\n        X,sr = librosa.load(fp)\n        raw_sounds.append(X)\n    return raw_sounds\n\ndef plot_waves(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,60), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(10,1,i)\n        librosa.display.waveplot(np.array(f),sr=22050)\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Figure 1: Waveplot',x=0.5, y=0.915,fontsize=18)\n    plt.show()\n    \ndef plot_specgram(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,60), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(10,1,i)\n        specgram(np.array(f), Fs=22050)\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Figure 2: Spectrogram',x=0.5, y=0.915,fontsize=18)\n    plt.show()\n\ndef plot_log_power_specgram(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,60), dpi = 900)\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(10,1,i)\n        D = librosa.logamplitude(np.abs(librosa.stft(f))**2, ref_power=np.max)\n        librosa.display.specshow(D,x_axis='time' ,y_axis='log')\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Figure 3: Log power spectrogram',x=0.5, y=0.915,fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"ndXOjzE0tvGT"},"cell_type":"markdown","source":"# Extracting features"},{"metadata":{"colab":{},"colab_type":"code","id":"CRakr1AetvGW","trusted":true},"cell_type":"code","source":"def extract_feature(file_name):\n    X, sample_rate = librosa.load(file_name)\n    stft = np.abs(librosa.stft(X))\n    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n    #print(chroma.shape)\n    #print(\"chroma\")\n    #print(mfccs.shape)\n    #print(\"mfccs\")\n    #print(mel.shape)\n    #print(\"mels\")\n    #print(contrast.shape)\n    #print(\"contrast\")\n    #print(tonnetz.shape)\n    #print(\"tonnetz\")    \n    return mfccs,chroma,mel,contrast,tonnetz\n\ndef parse_audio_files(parent_dir,sub_dirs,file_ext='*.wav'):\n    d=0\n    duration1 = []\n    duration2 = []\n    for label, sub_dir in enumerate(sub_dirs):\n        for fn in glob.glob(os.path.join(parent_dir,sub_dir,file_ext)):\n            with contextlib.closing(wave.open(fn,'r')) as f:\n                frames = f.getnframes()\n                rate = f.getframerate()\n                duration = frames / float(rate)\n                print(pd.Series(fn.split('_')).unique())\n                print(d,duration)\n                d=d+1\n                print(\" \")\n                #dn=0\n                if(sub_dir!='noise_set'):\n                    #if(duration<0):\n                    #    dn =dn+1\n                    duration1.append(duration)  \n                else:\n                    duration2.append(duration)\n            \n    return duration1,duration2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_audio_files2(parent_dir,sub_dirs,file_ext='*.wav'):\n    features = np.empty((0,193))\n    finlabel = np.empty((0,3))\n    labells=[]\n    labells1=np.empty(0) \n    labells2=np.empty(0) \n    d=0\n    fns = []\n    labels=[]\n    label1s=[]\n    label2s=[]\n    for label, sub_dir in enumerate(sub_dirs):\n        for fn in glob.glob(os.path.join(parent_dir,sub_dir,file_ext)):\n            mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)\n            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n          #  print(\"ext_features\")\n          #  print(ext_features.shape)\n            features = np.vstack([features,ext_features])\n          #  print(\"features\")\n          #  print(features.shape)\n            if(sub_dir!='noise_set'):\n                    d = d+1\n                    if (len(fn.split('_'))>=6):\n                        label1 = \"1\"\n                    else:\n                        label1=\"0\"\n                    label=\"1\"\n                    if(sub_dir=='TRIM_old'):\n                        if(fn.split('_')[-1]=='2.wav' ):\n                            label2=\"2\"\n                        if(fn.split('_')[3]=='Multiplepeacock'):\n                            label2 = \"4\"\n                        else:\n                            label2 =\"1\"\n                    else:\n                        if(fn.split('_')[-1]=='1.wav'):\n                            label2=\"1\"\n                        elif(fn.split('_')[-1]=='2.wav'):\n                            label2 =\"2\" \n                        elif(fn.split('_')[-1]=='3.wav'):\n                            label2 =\"3\"\n                        elif(fn.split('_')[-1]=='MI.wav' or fn.split('_')[-1]=='3or4.wav'):\n                            label2 =\"4\"                        \n                        else:\n                            label2 =\"0\"\n                            \n            else:\n                label =\"0\"\n                label1=\"1\"\n                label2 =\"0\"\n            fns.append(fn.split('_')[1:])\n            use = np.hstack([label,label1,label2])\n            finlabel = np.vstack([finlabel,use])\n            labels.append(label)\n            label1s.append(label1)\n            label2s.append(label2)\n    return(fns, np.array(features), np.array(finlabel),np.array(labels), np.array(label1s), np.array(label2s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #  Feed Train & Test Data "},{"metadata":{"colab":{},"colab_type":"code","id":"yi23reLvtvGh","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parent_dir = '../input/data-w/data/data'\n\nsub_dirs = ['C_new2','TRIM_old','A_new1','trimmed_new3','noise_set','Chincholi','iid','Kaan'] #train dirs\n#sub_dirs2=['trimmed_new3','audio']        #test dirs\n\n#features, labels,la1 , la2,la3 = \nduration1,duration2= parse_audio_files(parent_dir,sub_dirs)\n#fea2,la2 = parse_audio_files(parent_dir,sub_dirs2)\n#print(features)\n#print(features.shape)\nprint('hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii')\n#print(labels)\n#print(fea2)\n#print(la2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dafr = pd.DataFrame()\ndafr[\"peacock_audio_durations\"] = pd.Series(duration1)\ndafr[\"noise_durations\"] = pd.Series(duration2)\n#dafr[\"index\"] = pd.Series(np.arange(0,78))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nax = sns.violinplot(data=dafr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fn,features,final_labels,label,label1,label2 = parse_audio_files2(parent_dir,sub_dirs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer,LabelBinarizer\nfrom numpy import argmax\nfrom keras.utils import to_categorical\nfrom keras import initializers\n\none_hot1 = LabelBinarizer()\n# One-hot encode data\nlabel_ = one_hot1.fit_transform(label)\nlabel_1 = one_hot1.fit_transform(label1)\n\nprint(one_hot1.classes_)\nprint(label_)\nprint(label_1)\n\none_hot2 = MultiLabelBinarizer()\n# One-hot encode data\nlabel_2 = one_hot2.fit_transform(label2)\nlabel__2 = to_categorical(label2)\nprint(label__2)\none_hot2.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(label__2.shape)\nprint(label_1.shape)\nprint(label_.shape)\nprint(label_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dafr_labels = pd.DataFrame()\ndafr_labels[\"file_name\"] = fn\ndafr_labels[\"peacock\"] = label_\ndafr_labels[\"disturbance\"] = label_1\n#dafr_labels[\"single_peacock\"] = [label__2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dafr_labels['peacock'].value_counts().plot(kind='bar')\n            #,'disturbance','single_peacock']].plot(kind='bar')\n#plt.xticks(np.arange(0, 51, 5)) \nplt.yticks(np.arange(0, 1200, 50)) \nplt.show()\ndafr_labels['disturbance'].value_counts().plot(kind='bar')\nplt.yticks(np.arange(0, 1200, 50)) \nplt.show()\n#dafr_labels['single_peacock'].value_counts().plot(kind='bar')\n#plt.yticks(np.arange(0, 1200, 25)) \n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(label).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(label1).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(label2).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dafr_labels","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Aj-3YAWttvGr"},"cell_type":"markdown","source":"# MLP keras "},{"metadata":{"colab":{},"colab_type":"code","id":"Kgb0pURqtvGt","outputId":"0ea78ba2-b274-4ccb-92dd-c310ad913d73","trusted":true},"cell_type":"code","source":"import keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Flatten, Dropout, BatchNormalization\n\nfrom sklearn.preprocessing import StandardScaler\n#from keras import metrics\nfrom sklearn.model_selection import train_test_split\n#from sklearn.metrics import f1_score\n#from keras.utils import to_categorical\nfrom keras.layers import Conv2D, MaxPooling2D\n#from keras import regularizers, optimizer\nimport pandas as pd\nimport random\nimport tensorflow as tf\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n#from sklearn.preprocessing import LabelEncoder\n#from sklearn.pipeline import Pipeline\nfrom keras import backend as K\n\n\nrandom.seed(42)\nnp.random.seed(42)\ntf.set_random_seed(42)\n\nmy_init = initializers.glorot_uniform(seed=42)\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"OYlHjiYWtvG1","trusted":true},"cell_type":"code","source":"#from keras.layers.core import Flatten\nX_train, X_test, y1_train, y1_test,y2_train,y2_test,y3_train,y3_test = train_test_split(features,\n                                            label_,label_1,label__2, test_size=0.33, random_state=42)\n#train_test_split(X, Y1, Y2, Y3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y3_train.shape)\nprint(y3_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"PRoOmMOetvG8","trusted":true},"cell_type":"code","source":"def plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    \n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    \n    ## Loss\n    plt.figure(1)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    ## Accuracy\n    plt.figure(2)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"0lzGtpAMtvHE"},"cell_type":"markdown","source":"## Multi output neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the model\ninn = Input(shape = (193,))\nx = Sequential()(inn)\n    # Adding the input layer and the first hidden layer\n#x =\nx = Dense(output_dim = 40, init = 'uniform', activation = 'relu', input_dim = 193)(x)\n    # Adding the second hidden layer\n#x = \nx=Dense(output_dim = 20, init = 'uniform', activation = 'relu')(x)\n#x =\nx= Dense(output_dim = 8, init = 'uniform', activation = 'relu')(x)\n    # Adding the output layer\nout1 =Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid')(x)\nout2 =Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid')(x)\nout3 =Dense(output_dim = 5, init = 'uniform', activation = 'softmax')(x)\n    # Compiling Neural Network\nmodel = Model(inputs=[inn], outputs=[out1, out2,out3])\n#classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# compile the model\nmodel.compile(optimizer='adam', loss=['binary_crossentropy','binary_crossentropy','categorical_crossentropy'],\n              metrics=[f1_m,f1_m,f1_m])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nhistory = model.fit(X_train, [y1_train,y2_train,y3_train],\n                    validation_split=0.3, epochs=100,batch_size = 10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\nf1score = model.evaluate(X_test, [y1_test,y2_test,y3_test])\n#classifier =KerasClassifier(build_fn=making_model(193), epochs=100, batch_size=10, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,precision_score\n,recall_score,f1_score,cohen_kappa_score,multilabel_confusion_matrix\nmultilabel_confusion_matrix(X_test, [y1_test,y2_test,y3_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_split_indices = np.random.rand(len(features))<0.67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(train_test_split_indices).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test data\nX_train = features[train_test_split]\nX_test  = features[~train_test_split]\ny_train = [label_[train_test_split],label_1[train_test_split],label__2[train_test_split]]\ny_test  = [label_[~train_test_split],label_1[~train_test_split],label__2[~train_test_split]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visible = Input(shape=input_shape)\nx1 = Conv2D(10, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(visible)\nx1 = MaxPooling2D(pool_size=(2, 2))(x1)\nx2 = Conv2D(20, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(x1)\nx2 = MaxPooling2D(pool_size=(2, 2))(x2)\nx3 = Conv2D(30, kernel_size=3, activation='elu', padding='same', kernel_initializer=my_init)(x2)\nx = GlobalMaxPooling2D()(x3)\noutput = Dense(Y_train.shape[1], activation='softmax', name = 'emotion', kernel_initializer=my_init)(x)\noutput2 = Dense(Y_train2.shape[1], activation='sigmoid', kernel_initializer=my_init)(GlobalMaxPooling2D()(x2))\noutput3 = Dense(Y_train3.shape[1], activation='linear', kernel_initializer=my_init)(GlobalMaxPooling2D()(x1))\nmodel = Model(inputs=visible, outputs=[output, output2, output3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(features, [label_,label_1,label__2], test_size = 0.33)\n#print(X_train.shape)\n#print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_ =  np.asarray(label_)\nprint(label_.shape)\nlabel_1 =  np.asarray(label_1)\nprint(label_1.shape)\nlabel__2 =  np.asarray(label__2)\nprint(label__2.shape)\nl = [label_,label_1,label__2]\nnp.asarray(l).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = making_model(193)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.vstack(label_,label_1,label__2).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.keras import BalancedBatchGenerator\nfrom sklearn.metrics import roc_auc_score\n\ndef fit_predict_balanced_model(X_train, y_train, X_test, y_test):\n    model = classifier\n    training_generator = BalancedBatchGenerator(X_train, y_train)\n    model.fit_generator(generator=training_generator,epochs=100,verbose=1)\n    y_pred = model.predict_proba(X_test)\n    print(classification_report(y_test, y_pred))\n    return roc_auc_score(y_test, y_pred)\nres = fit_predict_balanced_model(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"7qAVm6BttvHG","outputId":"5305c979-f605-4363-8849-d9f53d765ae4","trusted":true},"cell_type":"code","source":"# Fitting our model \nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nhistory=model.fit(X_train, y_train,validation_data=(X_test, y_test),nb_epoch = 100)\n#history=classifier.fit(X,y,validation_data=(X_test, y_test), batch_size = 10, nb_epoch = 100)\n#acc = []\n# Predicting the Test set results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[:,0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n#print(pd.Series(y_pred[:,0]).value_counts())\n#print(pd.Series(y_pred[:,1]).value_counts())\n#print(pd.Series(y_pred[:,2]).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = (y_pred > 0.5)\nprint('hiiiiii')\n#print(y_test)\n#print(y_pred)\nerr1=0\ncorr1=0\ncorr2=0\ncorr3=0\nerr2=0\nerr3=0\nfor i in range(len(y_test)):\n    for j in range(3):\n        if(y_pred[i][j]>=1.5 and y_pred[i][j]< 2.5):\n            y_pred[i][j]=2\n        elif(y_pred[i][j]>0.5 and y_pred[i][j]< 1.5):\n            y_pred[i][j]=1\n        elif(y_pred[i][j]>=2.5 and y_pred[i][j]< 3.5):\n            y_pred[i][j]=3\n        elif(y_pred[i][j]>=3.5 and y_pred[i][j]< 4.5):\n            y_pred[i][j]=4\n        else:\n            y_pred[i][j]=0\nfor i in range(len(y_test)):            \n    if(y_test[i][0]!=y_pred[i][0]):\n        err1 = err1+1\n        print(y_test[i][0],y_pred[i][0])\n    else:\n        corr1=corr1+1\nprint(\"______________________________________________________\")\nfor i in range(len(y_test)):\n    if(y_test[i][1]!=y_pred[i][1]):\n        err2 = err2+1\n        print(y_test[i][1],y_pred[i][1])\n    else:\n        corr2=corr2+1\nprint(\"______________________________________________________\")\nfor i in range(len(y_test)):\n    if(y_test[i][2]!=y_pred[i][2]):\n        err3 = err3+1\n        print(y_test[i][2],y_pred[i][2])\n    else:\n        corr3=corr3+1\nprint(\"______________________________________________________\")\nprint('error')\nprint(err1)\nprint(corr1)\nprint(err2)\nprint(corr2)\nprint(err3)\nprint(corr3)\nscores = model.evaluate(x=X_test, y=y_test, batch_size=None, verbose=2, sample_weight=None, steps=None)\nprint('Test loss:', scores)\n#acc.append(accuracy)\n# evaluate the model\n#scores = model.evaluate(X[test], Y[test], verbose=0)\n#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n#cvscores.append(scores[1] * 100)\n#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test[0],y_pred[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test[1],y_pred[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test[2],y_pred[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perm = PermutationImportance(model,scoring='accuracy', random_state=1).fit(X_train,y_train[0])\n#eli5.show_weights(perm, feature_names = X_train.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"lETOhRePtvHO","outputId":"b0d24e76-1a03-40b6-b5aa-9fe1132928fc","trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"vnoW7B0ZtvHV","outputId":"6b0e124b-df64-4eb2-f478-442f750882fe","trusted":true},"cell_type":"code","source":"# Creating the Confusion Matrix\n\n#y_test = np.transpose()\ncm = confusion_matrix(y_test[:,0], y_pred[:,0])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"qo1755LFtvHb","outputId":"9e375ee8-24e1-430b-fb38-a20c4debbaaf","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,cohen_kappa_score\n#y_test = np.transpose()\ncm = confusion_matrix(y_test[:,1], y_pred[:,1])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"VVKM7pgztvHj","outputId":"6d83e947-fd32-4057-9d74-a2f30f5cc96e","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,cohen_kappa_score\n#y_test = np.transpose()\ncm = confusion_matrix(y_test[:,2], y_pred[:,2])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"sI-xSKHJtvHq","outputId":"54be4743-cae2-4516-db1b-7e245cda2983","trusted":true},"cell_type":"code","source":"f1_score(y_test[:,0],y_pred[:,0])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"ow3I59RgtvHw","outputId":"97d2ed1f-fa8d-4bb9-ff01-9a248e81d841","trusted":true},"cell_type":"code","source":"# F1 score\nf1_score(y_test[:,1],y_pred[:,1])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"6ApZVcmotvH2","outputId":"d22970f2-e239-4658-dbdd-47e50794b38e","trusted":true},"cell_type":"code","source":"f1_score(y_test[:,2],y_pred[:,2])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"JlH5MVk2tvH6","outputId":"dcc1108e-be47-4eba-ed06-3835f493e9c6","trusted":true},"cell_type":"code","source":"# Precision \nprecision_score(y_test[:,0], y_pred[:,0])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"focFWytktvID","outputId":"037481b1-45c7-47f8-fe99-d88ad9ae9cde","trusted":true},"cell_type":"code","source":"# Precision \nprecision_score(y_test[:,1], y_pred[:,1])","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"QqIDZmr1tvIH","outputId":"84a7ac83-3db4-4d78-a1f0-8c0bde0af03a","trusted":true},"cell_type":"code","source":"# Precision \nprecision_score(y_test[:,2], y_pred[:,2])","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"A-cBzcOetvIN"},"cell_type":"markdown","source":"## Label-1 Individual Neural network (Presence of Peacock sound)"},{"metadata":{"colab":{},"colab_type":"code","id":"3kR2McQCtvIO","outputId":"c0aa0d8b-1e34-4bd6-f2fa-2b6e633d8ca4","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, la1, test_size = 0.25)\n\n\n#X_train,y_train = features,labels\n#X_test , y_test = fea2,la2\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n#Initializing Neural Network\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 40, init = 'uniform', activation = 'relu', input_dim = 193))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'relu'))\n\n\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n\n# Compiling Neural Network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting our model \nhistory1=classifier.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size = 10, nb_epoch = 100)\n\n#acc = []\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\ny_pred = (y_pred > 0.5)\nprint('hiiiiii')\nprint(y_test)\nprint(y_pred)\n\nerr2=0\nfor i in range(len(y_test)):\n    if((y_test[i]!=y_pred[i])):\n        err2 = err2+1;\nprint('error')\nprint(err2)\n\nscores = classifier.evaluate(x=X_test, y=y_test, batch_size=None, verbose=2, sample_weight=None, steps=None)\n\nprint('Test loss:', scores)\n\n#acc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"2RjC_xLTtvIT","outputId":"789b033a-9ef5-4f6e-d2d0-df1222c8b3e1","trusted":true},"cell_type":"code","source":"plot_history(history1)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"Gi9qnYT8tvIZ"},"cell_type":"markdown","source":"## Label-2 Individual Neural network (Presence of Disturbance)"},{"metadata":{"colab":{},"colab_type":"code","id":"K7ZGj5_BtvIb","outputId":"7cfd4776-3120-4cfd-c4d1-6c995d89bea0","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, la2, test_size = 0.25)\n\n\n#X_train,y_train = features,labels\n#X_test , y_test = fea2,la2\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n#Initializing Neural Network\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 40, init = 'uniform', activation = 'relu', input_dim = 193))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'relu'))\n\n\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n\n# Compiling Neural Network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting our model \nhistory2=classifier.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size = 10, nb_epoch = 150)\n\n#acc = []\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\ny_pred = (y_pred > 0.5)\nprint('hiiiiii')\nprint(y_test)\nprint(y_pred)\n\nerr3=0\nfor i in range(len(y_test)):\n    if((y_test[i]!=y_pred[i])):\n        err3 = err3+1;\nprint('error')\nprint(err3)\n\nscores = classifier.evaluate(x=X_test, y=y_test, batch_size=None, verbose=2, sample_weight=None, steps=None)\n\nprint('Test loss:', scores)\n\n#acc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"BXitENPLtvIi"},"cell_type":"markdown","source":"# Plots - Results"},{"metadata":{"colab":{},"colab_type":"code","id":"4yxBijBptvIk","outputId":"562b23ec-79bb-4b35-f416-c6dfe3d5dfc0","trusted":true},"cell_type":"code","source":"plot_history(history2)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"5Uy_c426tvIp","outputId":"42bc75fb-d960-4b0e-a859-86880a983ca8","trusted":true},"cell_type":"code","source":"# Creating the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,cohen_kappa_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"zEUMUfvTtvIs","outputId":"f50f257c-e4ec-4925-e1fd-65aebc7d0798","trusted":true},"cell_type":"code","source":"# Precision \nprecision_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"vLQzXKCitvIz","outputId":"07d5b224-c130-4ffd-d053-3b4ba3adf60d","trusted":true},"cell_type":"code","source":"# Recall\nrecall_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"VqKPIW_etvI3","outputId":"150d19c9-ee5c-4aa3-9946-f22f8f1d5ec0","trusted":true},"cell_type":"code","source":"# F1 score\nf1_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"sD2t3dMytvI-","outputId":"ecf4ac78-9082-4cea-f2b1-4acb518649c1","trusted":true},"cell_type":"code","source":"# Cohen's kappa\ncohen_kappa_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"WAMP9LIBtvJC"},"cell_type":"markdown","source":"## Label-3 Individual Neural network (Presence of Multiple peacocks)"},{"metadata":{"colab":{},"colab_type":"code","id":"0uLY7nhTtvJD","outputId":"9650bf62-4d89-4890-fdaa-30aff8c8586b","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, la3, test_size = 0.25)\n\n\n#X_train,y_train = features,labels\n#X_test , y_test = fea2,la2\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n#Initializing Neural Network\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 40, init = 'uniform', activation = 'relu', input_dim = 193))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'relu'))\n\n\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n\n# Compiling Neural Network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting our model \nhistory3=classifier.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size = 10, nb_epoch = 150)\n\n#acc = []\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\ny_pred = (y_pred > 0.5)\nprint('hiiiiii')\nprint(y_test)\nprint(y_pred)\n\nerr3=0\nfor i in range(len(y_test)):\n    if((y_test[i]!=y_pred[i])):\n        err3 = err3+1;\nprint('error')\nprint(err3)\n\nscores = classifier.evaluate(x=X_test, y=y_test, batch_size=None, verbose=2, sample_weight=None, steps=None)\n\nprint('Test loss:', scores)\n\n#acc.append(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"bAb8Aol3tvJK","outputId":"5c63aadd-0afe-4a08-bd6a-74d149acb9c7","trusted":true},"cell_type":"code","source":"plot_history(history3)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"TmEi5JPKtvJP","outputId":"ac6b9815-093f-4b05-fd73-d26419b97f87","trusted":true},"cell_type":"code","source":"# Creating the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,cohen_kappa_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"WB6G9egCtvJT","outputId":"5b16db9d-22b6-4140-b817-59fed87ab8e5","trusted":true},"cell_type":"code","source":"# Precision \nprecision_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"ziKk4LF9tvJW","outputId":"fce6f8e7-1d61-4170-813b-e52d47cdbef1","trusted":true},"cell_type":"code","source":"# Recall\nrecall_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"4iXz1qF3tvJY","outputId":"d094e5d8-49bb-4bc0-99bb-cdc04a39bda9","trusted":true},"cell_type":"code","source":"# F1 score\nf1_score(y_test,y_pred)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["A-cBzcOetvIN","Gi9qnYT8tvIZ","WAMP9LIBtvJC"],"name":"Copy of Final_MLP_run1.ipynb","provenance":[],"toc_visible":true,"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}